{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer_xray.record_utils import ActivationRecorder\n",
    "from transformer_xray.perturb_utils import register_pertubation_hooks\n",
    "from modular_transformers.models import components\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "import gc\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    text = open(f\"/om2/user/jackking/transformer_xray/data/texts/{data_name}.txt\", \"r\").read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path = \"gpt2\"):\n",
    "    if model_path == \"gpt2\":\n",
    "        orig_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    else:\n",
    "        orig_model = components.LM.from_pretrained(model_path)\n",
    "    return orig_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_activations(model, hook_locations, data_name):\n",
    "    activation_recorder = ActivationRecorder(model, hook_locations)\n",
    "    activation_recorder.register_recording_hooks()\n",
    "\n",
    "    data = load_data(data_name)\n",
    "\n",
    "    context_window = model.config.n_ctx\n",
    "    input = tokenizer.encode(data, return_tensors=\"pt\")[:, :context_window]\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    logits = output.logits\n",
    "    activations = activation_recorder.get_activations()\n",
    "\n",
    "    return activations, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orthogonal_vector(v):\n",
    "    if np.all(v == 0):\n",
    "        raise ValueError(\"The input vector cannot be the zero vector.\")\n",
    "    # Create a matrix with the input vector as the first row\n",
    "    # and fill the rest with random values\n",
    "    A = np.vstack([v, np.random.rand(len(v)-1, len(v))])\n",
    "    # Use the null space to find a vector orthogonal to the input vector\n",
    "    u = np.linalg.svd(A)[2][-1]\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orthog_pcas(pcas, num_layers, num_components):\n",
    "    orthog_pcas = {layer: [] for layer in range(num_layers)}\n",
    "    token_num = len(pcas[0])\n",
    "    for layer in range(num_layers):\n",
    "        for token in range(token_num):\n",
    "            pc = pcas[layer][token].components_[:num_components, :]\n",
    "            pc = pc.mean(axis=0)\n",
    "            pc = pc / np.linalg.norm(pc)\n",
    "            orthog_pc = get_orthogonal_vector(pc)\n",
    "            orthog_pcas[layer].append(orthog_pc)\n",
    "    \n",
    "    return orthog_pcas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    try:\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'], encoding='utf-8')\n",
    "        # Convert the output into a list of integers for each GPU\n",
    "        memory_usage = [int(x) for x in result.strip().split('\\n')]\n",
    "        return memory_usage\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Failed to run nvidia-smi: \", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_bigrams(model, bigrams):\n",
    "    completions = []\n",
    "    for bigram in bigrams:\n",
    "        input = torch.tensor(tokenizer.encode(bigram)).to(device)\n",
    "        output = model(input)\n",
    "        prediction = output.logits[-1, :].argmax(dim=-1)\n",
    "        tokenized = tokenizer.decode(prediction)\n",
    "        completions.append(tokenized)\n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_divergence(act1, act2):\n",
    "    cosine_difs = torch.nn.functional.cosine_similarity(act1, act2)\n",
    "    return cosine_difs.mean().item()\n",
    "\n",
    "def distance_divergence(act1, act2):\n",
    "    difference = act1 - act2\n",
    "    dist = torch.norm(difference, p=2, dim=1).mean()\n",
    "    return dist.item()\n",
    "\n",
    "def get_KL_logit_divergence(perturbed_logits, orig_logits):\n",
    "    KL_divergence = torch.nn.functional.kl_div(torch.nn.functional.log_softmax(perturbed_logits, dim=-1), torch.nn.functional.softmax(orig_logits, dim=-1), reduction='batchmean')\n",
    "    return KL_divergence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca(activations):\n",
    "    activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "hook_locations = {\"all\": [\"before_attn\"]}\n",
    "orig_activations, orig_logits = make_activations(model, hook_locations, \"alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_loc = \"before_attn\"\n",
    "num_bigrams = 100\n",
    "\n",
    "orig_model = load_model(\"gpt2\")\n",
    "orig_model.to(device)\n",
    "orig_model.eval()\n",
    "num_layers = len(orig_model.transformer.h)\n",
    "\n",
    "hook_locations = {\"all\": [perturbation_loc]}\n",
    "orig_activations, orig_logits = make_activations(orig_model, hook_locations, num_bigrams)\n",
    "pcas = get_pcas(orig_activations, orig_model, perturbation_loc)\n",
    "orig_activations = get_activations_matrix(num_layers, orig_activations)\n",
    "\n",
    "#iter starts here\n",
    "\n",
    "perturbation_size = 0.01\n",
    "num_components = 10\n",
    "name = \"orthogonal_pc\"\n",
    "orthog_pcas = get_orthog_pcas(pcas, num_layers, num_components)\n",
    "\n",
    "layers = [i for i in range(len(orig_model.transformer.h))]\n",
    "\n",
    "act_save_path = f\"/om2/user/jackking/MyData/dynamics/activations/{perturbation_loc}/{name}/{perturbation_size}/\"\n",
    "save_path = f\"/om2/user/jackking/transformer_xray/data/{perturbation_loc}/{name}/{perturbation_size}/\"\n",
    "if not os.path.exists(act_save_path):\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(act_save_path)\n",
    "if not os.path.exists(save_path):\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "test_bigrams = json.load(open(\"/om2/user/jackking/transformer_xray/scripts/dynamics/test_bigrams.json\"))\n",
    "test_bigrams = test_bigrams[\"interesting\"] + test_bigrams[\"most_variance\"] + test_bigrams[\"least_variance\"]\n",
    "\n",
    "bigram_results_df = pd.DataFrame(columns=['bigram'] + [layer for layer in layers])\n",
    "bigram_results_df['bigram'] = test_bigrams\n",
    "\n",
    "column_names = [\"layer_num\"] + [\"cosine_lyapunov\", \"distance_lyapunov\", \"KL_logit_div\"] + [f\"cosine_sim_{i}\" for i in layers] + [f\"distance_{i}\" for i in layers] \n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "def perturbation_function(input, layer, token):\n",
    "    try:\n",
    "        orthog_pc = orthog_pcas[layer][token]\n",
    "    except:\n",
    "        orthog_pc = 0\n",
    "    return torch.tensor(orthog_pc) / np.linalg.norm(orthog_pc) * perturbation_size\n",
    "\n",
    "for layer in layers:\n",
    "    torch.cuda.empty_cache() \n",
    "    print(f'Running layer {layer}')\n",
    "    gpu_memory_usage = get_gpu_memory_usage()\n",
    "    print(\"GPU Memory Usage (in MB):\", gpu_memory_usage)\n",
    "\n",
    "    model = load_model(\"gpt2\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    num_layers = len(model.transformer.h)\n",
    "\n",
    "    perturbation_hooks = {\"all\": [(\"before_attn\", \"all\", perturbation_function)]}\n",
    "    register_pertubation_hooks(model, perturbation_hooks, device)\n",
    "\n",
    "    perturbed_activations, perturbed_logits = make_activations(model, hook_locations, num_bigrams)\n",
    "    perturbed_activations = get_activations_matrix(num_layers, perturbed_activations)\n",
    "\n",
    "    torch.save(perturbed_activations, f'{act_save_path}/{layer}_perturbed_activations.pt')\n",
    "\n",
    "    completions = complete_bigrams(model, test_bigrams)\n",
    "    bigram_results_df[layer] = completions\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    KL_logit_div = get_KL_logit_divergence(perturbed_logits, orig_logits)\n",
    "\n",
    "    if not layer == 11:\n",
    "        cosine_difs = []\n",
    "        distances = []\n",
    "        for compare_layer in layers[layer+1:]:\n",
    "            cosine_dif = 1 - cosine_divergence(orig_activations[compare_layer], perturbed_activations[compare_layer])\n",
    "            distance = distance_divergence(orig_activations[compare_layer], perturbed_activations[compare_layer])\n",
    "            cosine_difs.append(cosine_dif)\n",
    "            distances.append(distance)\n",
    "        cosine_lyapunov = np.log(np.abs(np.array(cosine_difs) + 1e-9 / perturbation_size)).sum() / len(cosine_difs)\n",
    "        distance_lyapunov = np.log(np.array(distances) + 1e-9 / perturbation_size).sum() / len(distances)\n",
    "        \n",
    "        new_df = pd.DataFrame({\n",
    "            \"layer_num\": [layer],\n",
    "            \"cosine_lyapunov\": [cosine_lyapunov],\n",
    "            \"distance_lyapunov\": [distance_lyapunov],\n",
    "            \"KL_logit_div\": [KL_logit_div],\n",
    "            **{f\"cosine_sim_{i}\": [cosine_difs[i - (layer + 1)]] for i in layers[layer+1:]},\n",
    "            **{f\"distance_{i}\": [distances[i - (layer + 1)]] for i in layers[layer+1:]}\n",
    "        })\n",
    "    else:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"layer_num\": [layer],\n",
    "            \"KL_logit_div\": [KL_logit_div],\n",
    "        })\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(f'{save_path}.csv')      \n",
    "    bigram_results_df.to_csv(f'{save_path}/bigram_results.csv')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
